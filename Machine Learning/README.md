# Machine Learning: Learner's Space

Hello everyone, welcome to the machine learning course offered by WnCC in association with Career Cell under Learner's Space. 

We will be covering the essential components of clasical machine learning before moving onto the basics of deep learning in this course. This course requires you to be comfortable with the basics of Python as a prerequisite. Follow the tutorials below to get started!

 - **[Week 1](./Week%201/README.md)**
 - **[Week 2](./Week%202/README.md)**
 - **[Week 3](./Week%203/README.md)**
 - **[Week 4](./Week%204/README.md)**
 - **[Mid-term Assignment](./MId%20Term%20Assignment/README.md)**
 - **[Week 5](https://github.com/wncc/learners-space/blob/master/Machine%20Learning/Week%205/README.md)**
 - **[Week 6](https://github.com/wncc/learners-space/blob/master/Machine%20Learning/Week%206/README.md)**
 - **[End Term Assignment](./End%20Term%20Assignment/README.md)**
 ***
 
## Week 1
 
We begin our journey in the world of Machine Learning by laying down the foundation stone of various associated domains of Mathematics and Coding skills. 

* We start with **Python**, probably one of the most popular languages around the globe used in Machine Learning and programming at large. 
* We then head on to one of the most important skills of Machine Learning namely **Data Plotting and Visualisation**. It is one of the first steps to solve a machine learning challenge. Unless one is familiar with the data he/she will deal with is essential to developing a good Machine Learning model for the task. 
* We talk about **Data Distribution** which is central to understanding the kind of model that can be used to solve the problem at hand.
* Let us say, we know what data we want to deal with and know about it's distribution but we generally can't feed the data directly to our model and hence it is essential to **prepare** the data and **pre-process** it as per need of our model. So, we look at a really useful library called Pandas.
* Having prepared the data, now the question is, how to process it efficiently. Here creeps in **Numpy - Numerical Python**. One of the most extensively used libaries for computing and widely used to speed up the task at hand.
* We now try to acclimatise readers with Machine Learning foundations namely, **Calculus and Linear Algebra**.

We are now set to dive deep into Machine learning and every aspect of it in detail starting with week 2.
 
## Week 2

We begin our walk with basics of Machine Learning i.e. Regression and Classification in this week.

* **Linear Regression** is probably one of the most fundamental ideas in Machine Learning and in fact, Machine Learning build around Linear Regression.
* A more efficient model than simple Linear Regression finds it's way onto the middle often referred to as **Segmented Regression**. We talk about the relative performance of this model wrt Simple Linear Regression. 
* A really good Linear Regression model based on really good observations is **Locally Weighted Regression**, which we talk in detail about.
* From the world of linearity in regression model, we jump into **Logistic Regression for Classification of Data**.
* We talk about a relatively advanced model called **Naive Bayes Classifier** which was used extensively in Machine Learning era.
* We then try to build upon the Data Analysis we learned in Week 1 by understanding **Exploratory Data Analysis** and **Data Pre-Processing**.
* We finally try to unify all the types of linear models under one umbrella called **Generalised Linear Models (GLM)**. This topic turns to be math intensive and so we in some sense try to make it as much as intutive as possible and avoided the math behind stuff.


## Week 3

From the realm of Linear Models, we try to explore some non-linear models in this week and understand some **Unsupervised Learning** models.

* We start with a non-parametric learning model called the **Decision Tree**. This is a really simple idea and the first glance may seem to be trivial but is a really powerful model for both **Regression** and **Classification**. It is really important to understand this well as this was used and is still in use extensively.
* We also talk about an Algorithm for construcyion of Decision Trees.
* We then talk about **Ensemble**. We talk about an ensemble of **Decision Trees** known as **Random Forest** and try to figure out the ways in which we should try to ensemble decision trees. We talk about pros and cons of this model relative to Decision Trees.
* How should we deal an image that has hundreds of dimension. Is it feasible to use the entire image in its original form in our conventional models? To answer such questions, we try to explain a technique called **Principal Component Analysis** that is used greatly for **Face Recognition**. This in essence is a **Dimensionality Reduction Technique**.
* We now, jump to an **Unsupervised Learning** model named **k Nearest Neighbours (kNN)** used to make some sense of the data by clustering the data. It is used widely today as a Classifier. It is essentially a **Non Parametric Learning** model.

## Week 4

This week we take a flavour of **Neural Networks** that have revolutionised the field of Artifical Intelligence. It is used extensively in almost every domain of Artifical Learning. With it's advent, the conventional learning models lost their importance because in some sense this unifies everything into one and only one model that can do almost everything that the machine learning models do. 

* We talk about the fundamentals of **Neural Nets** and the associated mathematics for a better understanding of it's working.
* We also talk about few general concepts like **Loss Function** and **Optimisation techniques**.
* From this week onwards, we also start to look at **practical aspects** of Machine Learning models which are undoubtedly the determiners of efficiency of a model. They are crucial things to which one must arrive at before diving into to solving the task at hand.
* We also introduce some **statistical** concepts that are closely related to Machine Learning and are fundamental to the understanding of Machine learning models.
* We also give a few practice exercises to get your understanding clear on this really important model.

## Week 5

This week, we start with implementation details of Neural Networks in Standard Libraries like Tensorflow and Keras. We also try to further our knowledge of Neural Nets with more advanced concepts.

* We start with tutorials on **tensorflow** and **Keras** where we learn about the implementation of Neural Nets and applying them to tasks.
* We then head on to Convolutional Neural Nets. Probaby one of the greatest developments in the field of Deep Learning that has indeed changed the field of Computer Vision completely. 
* We talk a lot of detail about CNN's and introduce you to **Conv Layers** and **Pooling Layers**.
* A few exercises have been given to acquaint readers with Neural Nets and Convolutional Neural Nets.

## Week 6

In this final week, we talk a lot about practical details of Neural Nets and how they are crucial in determining the performance of our models.

* We start with **Initialisation of Parameters** and talk extensively of it's effect on learning process of a model.
* We try to unify all the techniques into one standard expression of Standard Deviation as explained in a Research Paper added in the content section.
* We head on to essentially our last topic of discussion i.e. **Dropouts**. Google came up with this wonderful idea of Dropouts that in some sense allowed an ensemble using a single neural net and this improved performance greatly.

With this we come to the end of our course that was developed completely by the members of the **WnCC  Family** with great efforts and an ambition to give the reader a tour of the really amazing Machine Learning field and acquaint you with the potential of Machine Learning to revolutionise the era of technology and make this world even more exciting.

---
 
<p align="center">Created with :heart: by <a href="https://www.wncc-iitb.org/">WnCC</a></p>

